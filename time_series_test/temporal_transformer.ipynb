{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc93f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\agentic\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Device: cuda\n",
      "Forecast horizon: 26 steps (13.0 hrs)\n",
      "üìÇ Loading: history_data\\history_data_FORCEMOT\\FORCEMOT_30min_2024-11-03_to_2025-11-03.csv\n",
      "Training cutoff index: 947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\agentic\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "d:\\ANACONDA\\envs\\agentic\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† TFT parameters: 107.4k\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    118\u001b[39m checkpoint = ModelCheckpoint(dirpath=MODEL_DIR, filename=\u001b[33m\"\u001b[39m\u001b[33mtft_best\u001b[39m\u001b[33m\"\u001b[39m, monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m trainer = Trainer(\n\u001b[32m    121\u001b[39m     max_epochs=EPOCHS,\n\u001b[32m    122\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEVICE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     log_every_n_steps=\u001b[32m10\u001b[39m\n\u001b[32m    127\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m best_model_path = checkpoint.best_model_path\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müèÅ Best model saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\envs\\agentic\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:553\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\n\u001b[32m    508\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    509\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     ckpt_path: Optional[_PATH] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    515\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    551\u001b[39m \n\u001b[32m    552\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     model = \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.strategy._lightning_module = model\n\u001b[32m    555\u001b[39m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m.strategy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\envs\\agentic\\Lib\\site-packages\\pytorch_lightning\\utilities\\compile.py:111\u001b[39m, in \u001b[36m_maybe_unwrap_optimized\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    110\u001b[39m _check_mixed_imports(model)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    112\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model).\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚ö° Temporal Fusion Transformer ‚Äî Multi-horizon Forecast\n",
    "# ‚úÖ 30-min OHLCV + indicators, no overnight gaps\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------- CONFIG ----------------------\n",
    "DATA_BASE_PATH = \"history_data\"\n",
    "STOCK_CODE = \"FORCEMOT\"\n",
    "TIMEFRAME = \"30min\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"tft_multistep_best.ckpt\")\n",
    "RESULT_FILE = \"tft_results.json\"\n",
    "\n",
    "LOOKBACK = 50           # encoder length\n",
    "FORECAST_HORIZON = 26   # decoder length\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "\n",
    "print(f\"‚öôÔ∏è Device: {DEVICE}\")\n",
    "print(f\"Forecast horizon: {FORECAST_HORIZON} steps ({FORECAST_HORIZON*30/60:.1f} hrs)\")\n",
    "\n",
    "# ---------------------- LOAD DATA ----------------------\n",
    "def load_csv(stock_code: str, tf: str):\n",
    "    folder = os.path.join(DATA_BASE_PATH, f\"history_data_{stock_code}\")\n",
    "    files = [f for f in os.listdir(folder) if tf in f and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV found for {stock_code}\")\n",
    "    path = os.path.join(folder, sorted(files)[-1])\n",
    "    print(\"üìÇ Loading:\", path)\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = load_csv(STOCK_CODE, TIMEFRAME)\n",
    "\n",
    "FEATURES = [\n",
    "    \"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\n",
    "    \"MA_Fast\",\"MA_Slow\",\"BB_Upper\",\"BB_Lower\",\n",
    "    \"MACD\",\"MACD_Signal\",\"MACD_Hist\",\"+DI\",\"-DI\",\"ADX\",\n",
    "    \"RSI14\",\"ATR14\",\"atr_pct\"\n",
    "]\n",
    "df = df.dropna(subset=FEATURES).reset_index(drop=True)\n",
    "\n",
    "# ---------------------- PREPROCESS ----------------------\n",
    "# Assign continuous time index ignoring gaps\n",
    "df[\"time_idx\"] = np.arange(len(df))\n",
    "df[\"stock_code\"] = STOCK_CODE  # group id\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# ---------------------- DATASET SPLIT ----------------------\n",
    "# we use last part as test\n",
    "max_encoder_length = LOOKBACK\n",
    "max_prediction_length = FORECAST_HORIZON\n",
    "\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length * 2\n",
    "print(f\"Training cutoff index: {training_cutoff}\")\n",
    "\n",
    "# ---------------------- TimeSeriesDataSet ----------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Close\",\n",
    "    group_ids=[\"stock_code\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=FEATURES,  # unknown future indicators\n",
    "    target_normalizer=None,\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df,\n",
    "    min_prediction_idx=training_cutoff + 1,\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "train_loader = training.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=2)\n",
    "val_loader = validation.to_dataloader(train=False, batch_size=BATCH_SIZE, num_workers=2)\n",
    "\n",
    "# ---------------------- MODEL ----------------------\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=5,\n",
    ")\n",
    "print(f\"üß† TFT parameters: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# ---------------------- TRAIN ----------------------\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "checkpoint = ModelCheckpoint(dirpath=MODEL_DIR, filename=\"tft_best\", monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, lr_logger, checkpoint],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_loader, val_loader)\n",
    "\n",
    "best_model_path = checkpoint.best_model_path\n",
    "print(f\"üèÅ Best model saved at: {best_model_path}\")\n",
    "\n",
    "# ---------------------- PREDICT ----------------------\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "pred_raw = best_tft.predict(val_loader, mode=\"raw\")\n",
    "pred_mean = best_tft.predict(val_loader)  # mean prediction\n",
    "\n",
    "# Extract actual targets (unscaled)\n",
    "y_true = torch.cat([y[1] for y in iter(val_loader)], dim=0).numpy()\n",
    "y_pred = pred_mean.numpy()\n",
    "\n",
    "# ---------------------- METRICS ----------------------\n",
    "# Compute per-horizon metrics\n",
    "mae_list, rmse_list, r2_list = [], [], []\n",
    "H = y_pred.shape[1]\n",
    "for h in range(H):\n",
    "    mae_list.append(mean_absolute_error(y_true[:, h], y_pred[:, h]))\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_true[:, h], y_pred[:, h])))\n",
    "    r2_list.append(r2_score(y_true[:, h], y_pred[:, h]))\n",
    "\n",
    "# Directional accuracy (first horizon)\n",
    "prev_closes = df[\"Close\"].iloc[-len(y_pred): -len(y_pred)+len(y_pred)].values\n",
    "pred_dir = np.sign(y_pred[:, 0] - prev_closes[:len(y_pred)])\n",
    "true_dir = np.sign(y_true[:, 0] - prev_closes[:len(y_true)])\n",
    "diracc = np.mean(pred_dir == true_dir)\n",
    "\n",
    "best_h = int(np.argmin(mae_list))\n",
    "print(f\"‚úÖ Best horizon: {best_h}, MAE={mae_list[best_h]:.3f}, RMSE={rmse_list[best_h]:.3f}, DirAcc={diracc:.3f}\")\n",
    "\n",
    "# ---------------------- SAVE RESULTS ----------------------\n",
    "results = {\n",
    "    \"Config\": {\"lookback\": LOOKBACK, \"horizon\": FORECAST_HORIZON, \"lr\": LR},\n",
    "    \"Metrics\": {\n",
    "        \"MAE_all\": mae_list,\n",
    "        \"RMSE_all\": rmse_list,\n",
    "        \"R2_all\": r2_list,\n",
    "        \"DirAcc_h0\": diracc,\n",
    "        \"Best_Horizon\": best_h,\n",
    "    }\n",
    "}\n",
    "with open(RESULT_FILE, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"üìÅ Results saved: {RESULT_FILE}\")\n",
    "\n",
    "# ---------------------- PLOTLY VISUALIZATION ----------------------\n",
    "timestamps = df[\"Timestamp\"].iloc[-len(y_true):].reset_index(drop=True)\n",
    "x_idx = np.arange(len(timestamps))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_idx, y=y_true[:, best_h], mode=\"lines+markers\", name=\"Actual Close\",\n",
    "    text=timestamps.dt.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "    hovertemplate=\"%{text}<br>Actual: %{y:.2f}<extra></extra>\"\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_idx, y=y_pred[:, best_h], mode=\"lines+markers\", name=\"Predicted Close\",\n",
    "    text=timestamps.dt.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "    hovertemplate=\"%{text}<br>Pred: %{y:.2f}<extra></extra>\"\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f\"{STOCK_CODE} ‚Äî Temporal Fusion Transformer (h={best_h})\",\n",
    "    xaxis_title=\"Index (gap-free)\", yaxis_title=\"Close Price\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
